import json
import os
import asyncio
import concurrent.futures
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Any
import pandas as pd
import numpy as np
import uuid
from dotenv import load_dotenv
from .hwb_data_manager import HWBDataManager
import logging
import warnings
from .rs_calculator import RSCalculator
from .image_generator import generate_stock_chart

warnings.filterwarnings("ignore")
logger = logging.getLogger(__name__)
load_dotenv()

# --- Constants ---
BATCH_SIZE = int(os.getenv('BATCH_SIZE', '50'))
MAX_WORKERS = int(os.getenv('MAX_WORKERS', '10'))

# Rule 1: Trend Filter
WEEKLY_TREND_THRESHOLD = float(os.getenv('WEEKLY_TREND_THRESHOLD', '0.0'))

# Rule 2: Setup
SETUP_LOOKBACK_DAYS = int(os.getenv('SETUP_LOOKBACK_DAYS', '30'))
INITIAL_SCAN_MIN_HISTORY_DAYS = int(os.getenv('INITIAL_SCAN_MIN_HISTORY_DAYS', '1000'))

# Rule 3: FVG Detection (bot_hwb.pyæ–¹å¼)
FVG_MIN_GAP_PERCENTAGE = float(os.getenv('FVG_MIN_GAP_PERCENTAGE', '0.001'))  # 0.1%
FVG_MAX_SEARCH_DAYS = int(os.getenv('FVG_MAX_SEARCH_DAYS', '20'))
PROXIMITY_PERCENTAGE = float(os.getenv('PROXIMITY_PERCENTAGE', '0.05'))  # 5%
FVG_ZONE_PROXIMITY = float(os.getenv('FVG_ZONE_PROXIMITY', '0.10'))  # 10%

# Rule 4: Breakout (bot_hwb.pyæ–¹å¼)
BREAKOUT_THRESHOLD = float(os.getenv('BREAKOUT_THRESHOLD', '0.001'))  # 0.1%


class HWBAnalyzer:
    """HWBåˆ†æã‚¨ãƒ³ã‚¸ãƒ³ï¼ˆbot_hwb.pyæ–¹å¼ã«çµ±ä¸€ï¼‰"""
    
    def __init__(self):
        self.market_regime = 'TRENDING'
        self.params = self._adaptive_parameters()

    def _adaptive_parameters(self):
        """å¸‚å ´ç’°å¢ƒã«å¿œã˜ãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´"""
        params = {
            'TRENDING': {
                'setup_lookback': 30, 
                'fvg_search_days': 20, 
                'ma_proximity': 0.05, 
                'breakout_threshold': 0.001  # å›ºå®š0.1%
            },
        }
        return params.get(self.market_regime, params['TRENDING'])

    def optimized_rule1(self, df_daily: pd.DataFrame, df_weekly: pd.DataFrame) -> bool:
        """Rule â‘ : é€±è¶³ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆç¾æ™‚ç‚¹ãƒã‚§ãƒƒã‚¯ç”¨ï¼‰"""
        if df_weekly is None or df_weekly.empty:
            return False
        if 'sma200' not in df_weekly.columns or df_weekly['sma200'].isna().all():
            return False

        latest_weekly = df_weekly.iloc[-1]
        if pd.isna(latest_weekly['sma200']) or latest_weekly['sma200'] == 0:
            return False

        weekly_deviation = (latest_weekly['close'] - latest_weekly['sma200']) / latest_weekly['sma200']
        return weekly_deviation >= WEEKLY_TREND_THRESHOLD

    def check_weekly_trend_at_date(self, df_weekly: pd.DataFrame, check_date: pd.Timestamp) -> bool:
        """ç‰¹å®šæ—¥æ™‚ç‚¹ã§ã®é€±è¶³ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼"""
        if df_weekly is None or df_weekly.empty:
            return False
        
        df_weekly_historical = df_weekly[df_weekly.index <= check_date]
        
        if df_weekly_historical.empty:
            return False
        
        if 'sma200' not in df_weekly_historical.columns or df_weekly_historical['sma200'].isna().all():
            return False
        
        latest_weekly_at_date = df_weekly_historical.iloc[-1]
        
        if pd.isna(latest_weekly_at_date['sma200']) or latest_weekly_at_date['sma200'] == 0:
            return False
        
        weekly_deviation = (
            (latest_weekly_at_date['close'] - latest_weekly_at_date['sma200']) 
            / latest_weekly_at_date['sma200']
        )
        
        return weekly_deviation >= WEEKLY_TREND_THRESHOLD

    def optimized_rule2_setups(
        self, 
        df_daily: pd.DataFrame, 
        df_weekly: pd.DataFrame,
        full_scan: bool = False,
        scan_start_date: Optional[pd.Timestamp] = None
    ) -> List[Dict]:
        """Rule â‘¡: ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¤œå‡ºï¼ˆé€±è¶³ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼çµ±åˆï¼‹å…¨æœŸé–“å¯¾å¿œç‰ˆï¼‰"""
        setups = []
        
        # ã‚¹ã‚­ãƒ£ãƒ³ç¯„å›²ã®æ±ºå®š
        if full_scan:
            scan_start_index = max(0, INITIAL_SCAN_MIN_HISTORY_DAYS)
            logger.info(f"ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¤œå‡ºï¼šå…¨æœŸé–“ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆ{scan_start_index}æ—¥ç›®ã€œ{len(df_daily)}æ—¥ç›®ï¼‰")
        elif scan_start_date:
            try:
                scan_start_index = df_daily.index.searchsorted(scan_start_date)
            except:
                scan_start_index = max(0, len(df_daily) - SETUP_LOOKBACK_DAYS)
        else:
            scan_start_index = max(0, len(df_daily) - SETUP_LOOKBACK_DAYS)
        
        if scan_start_index >= len(df_daily):
            return setups
        
        # ATRè¨ˆç®—ï¼ˆå…¨æœŸé–“ï¼‰
        atr = (df_daily['high'] - df_daily['low']).rolling(14).mean()
        
        for i in range(scan_start_index, len(df_daily)):
            row = df_daily.iloc[i]
            setup_date = df_daily.index[i]
            
            # ã“ã®æ—¥ä»˜æ™‚ç‚¹ã§é€±è¶³200MAãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
            if not self.check_weekly_trend_at_date(df_weekly, setup_date):
                continue
            
            if pd.isna(row.get('sma200')) or pd.isna(row.get('ema200')):
                continue

            # MAã‚¾ãƒ¼ãƒ³è¨ˆç®—
            zone_width = abs(row['sma200'] - row['ema200'])
            if atr.iloc[i] > 0:
                zone_width = max(zone_width, row['close'] * (atr.iloc[i] / row['close']) * 0.5)

            zone_upper = max(row['sma200'], row['ema200']) + zone_width * 0.2
            zone_lower = min(row['sma200'], row['ema200']) - zone_width * 0.2

            # ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—åˆ¤å®š
            if zone_lower <= row['open'] <= zone_upper and zone_lower <= row['close'] <= zone_upper:
                setup = {
                    'id': str(uuid.uuid4()),
                    'date': setup_date,
                    'type': 'PRIMARY',
                    'status': 'active',
                    'weekly_deviation': self._get_weekly_deviation_at_date(df_weekly, setup_date)
                }
                setups.append(setup)
            elif (zone_lower <= row['open'] <= zone_upper) or (zone_lower <= row['close'] <= zone_upper):
                body_center = (row['open'] + row['close']) / 2
                if zone_lower <= body_center <= zone_upper:
                    setup = {
                        'id': str(uuid.uuid4()),
                        'date': setup_date,
                        'type': 'SECONDARY',
                        'status': 'active',
                        'weekly_deviation': self._get_weekly_deviation_at_date(df_weekly, setup_date)
                    }
                    setups.append(setup)
        
        logger.info(f"ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¤œå‡ºå®Œäº†ï¼š{len(setups)}ä»¶")
        return setups

    def _get_weekly_deviation_at_date(self, df_weekly: pd.DataFrame, check_date: pd.Timestamp) -> Optional[float]:
        """æŒ‡å®šæ—¥æ™‚ç‚¹ã§ã®é€±è¶³200MAã‹ã‚‰ã®ä¹–é›¢ç‡ã‚’å–å¾—ï¼ˆè¨˜éŒ²ç”¨ï¼‰"""
        try:
            df_weekly_historical = df_weekly[df_weekly.index <= check_date]
            if df_weekly_historical.empty:
                return None
            
            latest = df_weekly_historical.iloc[-1]
            if pd.isna(latest['sma200']) or latest['sma200'] == 0:
                return None
            
            return (latest['close'] - latest['sma200']) / latest['sma200']
        except:
            return None

    def _check_fvg_ma_proximity(self, candle_3: pd.Series, candle_1: pd.Series) -> bool:
        """
        FVGãŒMAè¿‘æ¥æ¡ä»¶ã‚’æº€ãŸã™ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆbot_hwb.pyæ–¹å¼ï¼‰
        
        æ¡ä»¶A: 3æœ¬ç›®ã®å§‹å€¤orçµ‚å€¤ãŒMAÂ±5%ä»¥å†…
        æ¡ä»¶B: FVGã‚¾ãƒ¼ãƒ³ã®ä¸­å¿ƒãŒMAÂ±10%ä»¥å†…
        """
        if pd.isna(candle_3.get('sma200')) or pd.isna(candle_3.get('ema200')):
            return False
        
        # æ¡ä»¶A: 3æœ¬ç›®ã®å§‹å€¤orçµ‚å€¤ãŒMAÂ±5%ä»¥å†…
        for price in [candle_3['open'], candle_3['close']]:
            sma_deviation = abs(price - candle_3['sma200']) / candle_3['sma200']
            ema_deviation = abs(price - candle_3['ema200']) / candle_3['ema200']
            if sma_deviation <= PROXIMITY_PERCENTAGE or ema_deviation <= PROXIMITY_PERCENTAGE:
                return True
        
        # æ¡ä»¶B: FVGã‚¾ãƒ¼ãƒ³ã®ä¸­å¿ƒãŒMAÂ±10%ä»¥å†…
        fvg_center = (candle_1['high'] + candle_3['low']) / 2
        sma_deviation = abs(fvg_center - candle_3['sma200']) / candle_3['sma200']
        ema_deviation = abs(fvg_center - candle_3['ema200']) / candle_3['ema200']
        
        return sma_deviation <= FVG_ZONE_PROXIMITY or ema_deviation <= FVG_ZONE_PROXIMITY

    def optimized_fvg_detection(self, df_daily: pd.DataFrame, setup: Dict) -> List[Dict]:
        """
        Rule â‘¢: FVGæ¤œå‡ºï¼ˆbot_hwb.pyæ–¹å¼ã€ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å‰Šé™¤ï¼‰
        
        æ¡ä»¶:
        1. candle_3ã®low > candle_1ã®high (ã‚®ãƒ£ãƒƒãƒ—å­˜åœ¨)
        2. ã‚®ãƒ£ãƒƒãƒ—ç‡ > 0.1%
        3. MAè¿‘æ¥æ¡ä»¶ã‚’æº€ãŸã™
        """
        fvgs = []
        setup_date = setup['date']
        
        try:
            setup_idx = df_daily.index.get_loc(setup_date)
        except KeyError:
            return fvgs

        max_days = self.params['fvg_search_days']
        search_end = min(setup_idx + max_days, len(df_daily) - 1)

        for i in range(setup_idx + 2, search_end):
            if i >= len(df_daily):
                break
                
            candle_1 = df_daily.iloc[i-2]
            candle_3 = df_daily.iloc[i]
            
            # FVGæ¡ä»¶: candle_3ã®lowãŒcandle_1ã®highã‚ˆã‚Šä¸Š
            if candle_3['low'] <= candle_1['high']:
                continue

            # ã‚®ãƒ£ãƒƒãƒ—ç‡ãƒã‚§ãƒƒã‚¯ï¼ˆ0.1%ä»¥ä¸Šï¼‰
            gap_percentage = (candle_3['low'] - candle_1['high']) / candle_1['high']
            if gap_percentage < FVG_MIN_GAP_PERCENTAGE:
                continue

            # MAè¿‘æ¥æ¡ä»¶ãƒã‚§ãƒƒã‚¯ï¼ˆbot_hwb.pyæ–¹å¼ï¼‰
            if not self._check_fvg_ma_proximity(candle_3, candle_1):
                continue

            # FVGã¨ã—ã¦èªè­˜ï¼ˆã‚¹ã‚³ã‚¢ä¸è¦ï¼‰
            fvg = {
                'id': str(uuid.uuid4()),
                'setup_id': setup['id'],
                'formation_date': df_daily.index[i],
                'gap_percentage': gap_percentage,
                'lower_bound': candle_1['high'],
                'upper_bound': candle_3['low'],
                'status': 'active'
            }
            fvgs.append(fvg)
        
        return fvgs

    def optimized_breakout_detection_all_periods(
        self, 
        df_daily: pd.DataFrame, 
        setup: Dict, 
        fvg: Dict
    ) -> Optional[Dict]:
        """
        Rule â‘£: ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ¤œå‡ºï¼ˆbot_hwb.pyæ–¹å¼ã€ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å‰Šé™¤ï¼‰
        
        æ¡ä»¶:
        1. ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹ = ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã€œFVGé–“ã®æœ€é«˜å€¤
        2. çµ‚å€¤ > ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹ * (1 + 0.1%)
        3. FVGä¸‹é™ãŒç ´ã‚‰ã‚Œã¦ã„ãªã„
        """
        try:
            setup_idx = df_daily.index.get_loc(setup['date'])
            fvg_idx = df_daily.index.get_loc(fvg['formation_date'])
        except KeyError:
            return None

        # ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹ãƒ¬ãƒ™ãƒ«è¨ˆç®—ï¼ˆbot_hwb.pyæ–¹å¼ï¼šå˜ç´”ãªæœ€é«˜å€¤ï¼‰
        resistance_start_idx = setup_idx + 1
        resistance_end_idx = fvg_idx
        
        if resistance_end_idx <= resistance_start_idx:
            resistance_start_idx = max(0, setup_idx - 10)
            resistance_end_idx = setup_idx + 1
        
        resistance_data = df_daily.iloc[resistance_start_idx:resistance_end_idx]
        
        if resistance_data.empty:
            return None

        # ã‚·ãƒ³ãƒ—ãƒ«ãªæœ€é«˜å€¤ã‚’ãƒ¬ã‚¸ã‚¹ã‚¿ãƒ³ã‚¹ã¨ã™ã‚‹
        resistance_high = resistance_data['high'].max()

        # FVGé•åãƒã‚§ãƒƒã‚¯
        post_fvg_data = df_daily.iloc[fvg_idx:]
        if post_fvg_data['low'].min() < fvg['lower_bound'] * 0.98:
            return {
                'status': 'violated', 
                'violated_date': post_fvg_data['low'].idxmin()
            }

        # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯ï¼ˆFVGå½¢æˆæ—¥ã‹ã‚‰ç¾åœ¨ã¾ã§ã€å›ºå®šé–¾å€¤0.1%ï¼‰
        for i in range(fvg_idx + 1, len(df_daily)):
            current = df_daily.iloc[i]

            # bot_hwb.pyæ–¹å¼ï¼šå›ºå®šé–¾å€¤0.1%
            if current['close'] > resistance_high * (1 + BREAKOUT_THRESHOLD):
                breakout_date = df_daily.index[i]

                # å‡ºæ¥é«˜å¢—åŠ ç‡ã‚’è¨ˆç®—
                volume_metrics = self._calculate_volume_increase_at_date(df_daily, breakout_date)

                result = {
                    'status': 'breakout',
                    'breakout_date': breakout_date,
                    'breakout_price': current['close'],
                    'resistance_price': resistance_high,
                    'breakout_percentage': (current['close'] / resistance_high - 1) * 100
                }

                # å‡ºæ¥é«˜æƒ…å ±ã‚’è¿½åŠ 
                if volume_metrics:
                    result['breakout_volume'] = volume_metrics['breakout_volume']
                    result['avg_volume_20d'] = volume_metrics['avg_volume_20d']
                    result['volume_increase_pct'] = volume_metrics['volume_increase_pct']

                return result

        return None

    def _calculate_volume_increase_at_date(self, df_daily: pd.DataFrame, target_date: pd.Timestamp) -> Optional[Dict]:
        """
        ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ—¥ã®å‡ºæ¥é«˜å¢—åŠ ç‡ã‚’è¨ˆç®—ï¼ˆ20æ—¥å¹³å‡ã¨ã®æ¯”è¼ƒï¼‰

        Args:
            df_daily: æ—¥æ¬¡ãƒ‡ãƒ¼ã‚¿
            target_date: ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ—¥

        Returns:
            {
                'breakout_volume': ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ™‚ã®å‡ºæ¥é«˜,
                'avg_volume_20d': 20æ—¥å¹³å‡å‡ºæ¥é«˜,
                'volume_increase_pct': å¢—åŠ ç‡ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆï¼‰
            }
        """
        try:
            # volumeã‚«ãƒ©ãƒ ã®ç¢ºèª
            if 'volume' not in df_daily.columns:
                logger.warning(f"'volume' column not found in dataframe. Available columns: {df_daily.columns.tolist()}")
                return None

            # target_dateä»¥å‰ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            df_historical = df_daily[df_daily.index <= target_date].copy()

            # æœ€ä½21æ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦ï¼ˆ20æ—¥å¹³å‡ã‚’è¨ˆç®—ã™ã‚‹ãŸã‚ï¼‰
            if len(df_historical) < 21:
                logger.debug(f"Insufficient data for volume calculation at {target_date}")
                return None

            # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ—¥ã®å‡ºæ¥é«˜
            breakout_volume = df_historical.iloc[-1]['volume']

            # 20æ—¥å¹³å‡å‡ºæ¥é«˜ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ—¥ã®å‰æ—¥ã¾ã§ã®20æ—¥é–“ï¼‰
            avg_volume_20d = df_historical.iloc[-21:-1]['volume'].mean()

            if avg_volume_20d == 0 or pd.isna(avg_volume_20d):
                logger.warning(f"Invalid average volume at {target_date}")
                return None

            # å¢—åŠ ç‡ã‚’è¨ˆç®—ï¼ˆãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆï¼‰
            volume_increase_pct = ((breakout_volume / avg_volume_20d) - 1) * 100

            logger.debug(f"Volume increase at {target_date}: {volume_increase_pct:.1f}% (breakout: {breakout_volume:,.0f}, avg: {avg_volume_20d:,.0f})")

            return {
                'breakout_volume': int(breakout_volume),
                'avg_volume_20d': int(avg_volume_20d),
                'volume_increase_pct': round(volume_increase_pct, 1)
            }

        except Exception as e:
            logger.error(f"Error calculating volume increase: {e}", exc_info=True)
            return None


class HWBScanner:
    """ãƒ¡ã‚¤ãƒ³ã‚¹ã‚­ãƒ£ãƒŠãƒ¼ï¼ˆbot_hwb.pyæ–¹å¼ã«çµ±ä¸€ï¼‰"""
    
    def __init__(self):
        self.data_manager = HWBDataManager()
        self.analyzer = HWBAnalyzer()
        self.benchmark_df = None  # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥

    def _get_benchmark_data(self):
        """S&P500ï¼ˆSPYï¼‰ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã¨ã—ã¦å–å¾—"""
        if self.benchmark_df is not None:
            return self.benchmark_df

        try:
            logger.info("Loading S&P500 (SPY) benchmark data...")
            data = self.data_manager.get_stock_data_with_cache('SPY', lookback_years=10)
            if data:
                self.benchmark_df, _ = data
                logger.info(f"Benchmark data loaded: {len(self.benchmark_df)} days")
            return self.benchmark_df
        except Exception as e:
            logger.error(f"Failed to load benchmark data: {e}")
            return None

    def _calculate_rs_rating_at_date(self, df_daily: pd.DataFrame, target_date: pd.Timestamp) -> Optional[float]:
        """æŒ‡å®šæ—¥æ™‚ç‚¹ã§ã®RS Ratingã‚’è¨ˆç®—"""
        try:
            # âœ… ã‚«ãƒ©ãƒ åã®ç¢ºèª
            if 'close' not in df_daily.columns:
                logger.warning(f"'close' column not found in dataframe. Available columns: {df_daily.columns.tolist()}")
                return None

            benchmark_df = self._get_benchmark_data()
            if benchmark_df is None or 'close' not in benchmark_df.columns:
                logger.warning("Benchmark data not available or missing 'close' column")
                return None

            # target_dateä»¥å‰ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’ä½¿ç”¨
            df_historical = df_daily[df_daily.index <= target_date].copy()
            benchmark_historical = benchmark_df[benchmark_df.index <= target_date].copy()

            # æœ€ä½252æ—¥ã®ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
            if len(df_historical) < 252 or len(benchmark_historical) < 252:
                logger.debug(f"Insufficient data for RS calculation at {target_date}")
                return None

            # RSCalculatorã‚’ä½¿ç”¨ã—ã¦RS Ratingã‚’è¨ˆç®—
            rs_calc = RSCalculator(df_historical, benchmark_historical)
            rs_score_series = rs_calc.calculate_ibd_rs_score()
            current_rs_score = rs_score_series.iloc[-1]
            rs_rating = rs_calc.calculate_percentile_rating(current_rs_score)

            logger.debug(f"RS Rating calculated: {rs_rating:.0f}")
            return round(rs_rating)

        except Exception as e:
            logger.error(f"Error calculating RS rating: {e}", exc_info=True)
            return None

    async def scan_all_symbols(self, progress_callback=None):
        """å…¨ã‚·ãƒ³ãƒœãƒ«ã‚¹ã‚­ãƒ£ãƒ³"""
        symbols = list(self.data_manager.get_russell3000_symbols())
        total = len(symbols)
        logger.info(f"ã‚¹ã‚­ãƒ£ãƒ³é–‹å§‹: {total}éŠ˜æŸ„")
        scan_start_time = datetime.now()

        all_results = []
        processed_count = 0
        
        for i in range(0, total, BATCH_SIZE):
            batch = symbols[i:i + BATCH_SIZE]
            with concurrent.futures.ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                future_to_symbol = {
                    executor.submit(self._analyze_and_save_symbol, symbol): symbol 
                    for symbol in batch
                }
                for future in concurrent.futures.as_completed(future_to_symbol):
                    processed_count += 1
                    try:
                        result = future.result()
                        if result:
                            all_results.extend(result)
                    except Exception as exc:
                        logger.error(f"ã‚¨ãƒ©ãƒ¼: {future_to_symbol[future]} - {exc}", exc_info=True)
                    if progress_callback:
                        await progress_callback(processed_count, total)
            await asyncio.sleep(0.1)

        summary = self._create_daily_summary(all_results, total, scan_start_time)
        self.data_manager.save_daily_summary(summary)
        logger.info("ã‚¹ã‚­ãƒ£ãƒ³å®Œäº†")
        return summary

    def _analyze_and_save_symbol(self, symbol: str) -> Optional[List[Dict]]:
        """å˜ä¸€éŠ˜æŸ„åˆ†æï¼ˆçŠ¶æ…‹ãƒ™ãƒ¼ã‚¹å·®åˆ†å‡¦ç†ç‰ˆï¼‰"""
        try:
            data = self.data_manager.get_stock_data_with_cache(symbol)
            if not data:
                return None
            
            df_daily, df_weekly = data
            if df_daily.empty or df_weekly.empty:
                return None

            df_daily.index = pd.to_datetime(df_daily.index)
            df_weekly.index = pd.to_datetime(df_weekly.index)
            df_daily = df_daily[~df_daily.index.duplicated(keep='last')]
            df_weekly = df_weekly[~df_weekly.index.duplicated(keep='last')]

            latest_market_date = df_daily.index[-1].date()

            # Rule â‘ : ç¾æ™‚ç‚¹ã®ãƒˆãƒ¬ãƒ³ãƒ‰ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼ï¼ˆåˆæœŸãƒã‚§ãƒƒã‚¯ï¼‰
            if not self.analyzer.optimized_rule1(df_daily, df_weekly):
                return None

            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ç¢ºèª
            existing_data = self.data_manager.load_symbol_data(symbol)
            
            if existing_data:
                result = self._differential_analysis(symbol, df_daily, df_weekly, existing_data, latest_market_date)
            else:
                result = self._full_analysis(symbol, df_daily, df_weekly, latest_market_date)
            
            return result

        except Exception as e:
            logger.error(f"åˆ†æã‚¨ãƒ©ãƒ¼: {symbol} - {e}", exc_info=True)
            return None

    def _differential_analysis(self, symbol: str, df_daily: pd.DataFrame, df_weekly: pd.DataFrame,
                              existing_data: dict, latest_market_date: datetime.date) -> Optional[List[Dict]]:
        """å·®åˆ†åˆ†æï¼ˆRS Ratingè¿½åŠ ç‰ˆï¼‰"""
        existing_setups = existing_data.get('setups', [])
        existing_fvgs = existing_data.get('fvgs', [])
        existing_signals = existing_data.get('signals', [])
        
        for item in existing_setups + existing_fvgs + existing_signals:
            if 'date' in item:
                item['date'] = pd.to_datetime(item['date'])
            if 'formation_date' in item:
                item['formation_date'] = pd.to_datetime(item['formation_date'])
            if 'breakout_date' in item:
                item['breakout_date'] = pd.to_datetime(item['breakout_date'])
        
        active_setups = [s for s in existing_setups if s.get('status') == 'active']
        active_fvgs = [f for f in existing_fvgs if f.get('status') == 'active']
        
        last_analyzed_date = pd.to_datetime(existing_data.get('last_updated', '2000-01-01')).date()
        
        if latest_market_date <= last_analyzed_date:
            logger.debug(f"{symbol}: æ–°ã—ã„ãƒ‡ãƒ¼ã‚¿ãªã—")
            return self._create_summary_from_existing(existing_data, latest_market_date)
        
        logger.info(f"{symbol}: å·®åˆ†åˆ†æ ({last_analyzed_date} â†’ {latest_market_date})")
        
        updated = False
        new_fvgs_found = []
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã‹ã‚‰FVGæ¢ç´¢
        if active_setups:
            for setup in active_setups:
                setup_date = setup['date']
                setup_idx = df_daily.index.get_loc(setup_date)
                
                setup_fvgs = [f for f in existing_fvgs if f.get('setup_id') == setup['id']]
                if setup_fvgs:
                    last_fvg_date = max(f['formation_date'] for f in setup_fvgs)
                    search_start_date = last_fvg_date + pd.Timedelta(days=1)
                    search_start = df_daily.index.searchsorted(search_start_date)
                else:
                    search_start = setup_idx + 2
                
                search_end = min(setup_idx + FVG_MAX_SEARCH_DAYS, len(df_daily) - 1)
                new_data_start = df_daily.index.searchsorted(pd.Timestamp(last_analyzed_date) + pd.Timedelta(days=1))
                search_start = max(search_start, new_data_start)
                
                if search_start >= search_end:
                    continue
                
                new_fvgs = self._detect_fvg_in_range(df_daily, setup, search_start, search_end)
                
                if new_fvgs:
                    existing_data['fvgs'].extend(new_fvgs)
                    new_fvgs_found.extend(new_fvgs)
                    updated = True
        
        # ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯
        all_active_fvgs = active_fvgs + new_fvgs_found
        
        if all_active_fvgs:
            for fvg in all_active_fvgs:
                setup = next((s for s in existing_setups if s['id'] == fvg['setup_id']), None)
                if not setup or setup.get('status') == 'consumed':
                    continue
                
                fvg_date = fvg['formation_date']
                fvg_idx = df_daily.index.get_loc(fvg_date)
                new_data_start = df_daily.index.searchsorted(pd.Timestamp(last_analyzed_date) + pd.Timedelta(days=1))
                check_start = max(fvg_idx + 1, new_data_start)
                
                if check_start >= len(df_daily):
                    continue
                
                breakout = self._check_breakout_in_range(df_daily, setup, fvg, check_start, len(df_daily))
                
                if breakout and breakout.get('status') == 'breakout':
                    # âœ… RS Ratingã‚’è¨ˆç®—
                    breakout_date = pd.to_datetime(breakout['breakout_date'])
                    rs_rating = self._calculate_rs_rating_at_date(df_daily, breakout_date)

                    signal = {**fvg, **breakout}
                    if rs_rating is not None:
                        signal['rs_rating'] = rs_rating
                        logger.info(f"{symbol}: RS Rating at breakout = {rs_rating}")

                    existing_data['signals'].append(signal)
                    
                    setup['status'] = 'consumed'
                    for related_fvg in existing_data['fvgs']:
                        if related_fvg.get('setup_id') == setup['id']:
                            related_fvg['status'] = 'consumed'
                    
                    updated = True
                    break
                
                elif breakout and breakout.get('status') == 'violated':
                    fvg['status'] = 'violated'
                    fvg['violated_date'] = breakout.get('violated_date')
                    updated = True
        
        # æ–°è¦ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¢ç´¢
        if not active_setups or all(s.get('status') == 'consumed' for s in existing_setups):
            logger.info(f"{symbol}: æ–°ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—æ¢ç´¢")
            new_start_date = pd.Timestamp(last_analyzed_date) + pd.Timedelta(days=1)
            new_setups = self.analyzer.optimized_rule2_setups(df_daily, df_weekly, full_scan=False, scan_start_date=new_start_date)
            
            if new_setups:
                existing_data['setups'].extend(new_setups)
                updated = True
                logger.info(f"{symbol}: {len(new_setups)}ä»¶ã®æ–°ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—")
        
        if updated:
            existing_data['last_updated'] = datetime.now().isoformat()
            self._save_symbol_data_with_chart(symbol, existing_data, df_daily, df_weekly)
        
        return self._create_summary_from_existing(existing_data, latest_market_date)

    def _full_analysis(self, symbol: str, df_daily: pd.DataFrame, df_weekly: pd.DataFrame,
                      latest_market_date: datetime.date) -> Optional[List[Dict]]:
        """åˆå›ãƒ•ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆRS Ratingè¿½åŠ ç‰ˆï¼‰"""
        logger.info(f"{symbol}: åˆå›ãƒ•ãƒ«ã‚¹ã‚­ãƒ£ãƒ³ï¼ˆå…¨æœŸé–“ï¼š{len(df_daily)}æ—¥åˆ†ï¼‰")
        
        setups = self.analyzer.optimized_rule2_setups(df_daily, df_weekly, full_scan=True)
        
        if not setups:
            logger.info(f"{symbol}: ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ãªã—ï¼ˆå…¨æœŸé–“ï¼‰")
            return None

        consumed_setups = set()
        consumed_fvgs = set()
        all_fvgs = []
        all_signals = []

        for s in setups:
            s['date'] = pd.to_datetime(s['date'])

        for setup in setups:
            if setup['id'] in consumed_setups:
                setup['status'] = 'consumed'
                continue

            fvgs = self.analyzer.optimized_fvg_detection(df_daily, setup)
            signal_found_for_this_setup = False
            
            for fvg in fvgs:
                if fvg['id'] in consumed_fvgs:
                    fvg['status'] = 'consumed'
                    all_fvgs.append(fvg)
                    continue

                breakout = self.analyzer.optimized_breakout_detection_all_periods(df_daily, setup, fvg)

                if breakout:
                    if breakout.get('status') == 'breakout':
                        # âœ… RS Ratingã‚’è¨ˆç®—ï¼ˆãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆæ™‚ç‚¹ï¼‰
                        breakout_date = pd.to_datetime(breakout['breakout_date'])
                        rs_rating = self._calculate_rs_rating_at_date(df_daily, breakout_date)

                        signal = {**fvg, **breakout}
                        if rs_rating is not None:
                            signal['rs_rating'] = rs_rating
                            logger.info(f"{symbol}: RS Rating at breakout = {rs_rating}")
                        
                        all_signals.append(signal)
                        consumed_setups.add(setup['id'])
                        consumed_fvgs.add(fvg['id'])
                        fvg['status'] = 'consumed'
                        setup['status'] = 'consumed'
                        signal_found_for_this_setup = True
                        break
                    
                    elif breakout.get('status') == 'violated':
                        fvg['status'] = 'violated'
                        fvg['violated_date'] = breakout.get('violated_date')
                else:
                    fvg['status'] = 'active'
                
                all_fvgs.append(fvg)
            
            if not signal_found_for_this_setup:
                setup['status'] = 'active'

        if not all_signals and not any(f['status'] == 'active' for f in all_fvgs):
            logger.info(f"{symbol}: ã‚¢ã‚¯ãƒ†ã‚£ãƒ–ãªFVG/ã‚·ã‚°ãƒŠãƒ«ãªã—")
            return None

        def stringify_dates(d):
            for k, v in d.items():
                if isinstance(v, pd.Timestamp):
                    d[k] = v.strftime('%Y-%m-%d')
            return d

        symbol_data = {
            "symbol": symbol,
            "last_updated": datetime.now().isoformat(),
            "market_regime": self.analyzer.market_regime,
            "setups": [stringify_dates(s.copy()) for s in setups],
            "fvgs": [stringify_dates(f.copy()) for f in all_fvgs],
            "signals": [stringify_dates(s.copy()) for s in all_signals]
        }
        
        logger.info(
            f"{symbol}: å®Œäº† - ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—:{len(setups)}, "
            f"FVG:{len(all_fvgs)}, ã‚·ã‚°ãƒŠãƒ«:{len(all_signals)}"
        )
        
        self._save_symbol_data_with_chart(symbol, symbol_data, df_daily, df_weekly)
        return self._create_summary_from_data(symbol, all_signals, all_fvgs, latest_market_date)

    def _detect_fvg_in_range(self, df_daily: pd.DataFrame, setup: Dict, start_idx: int, end_idx: int) -> List[Dict]:
        """æŒ‡å®šç¯„å›²å†…ã§FVGæ¤œå‡ºï¼ˆbot_hwb.pyæ–¹å¼ï¼‰"""
        fvgs = []
        
        for i in range(start_idx, end_idx):
            if i < 2:
                continue
            
            candle_1 = df_daily.iloc[i-2]
            candle_3 = df_daily.iloc[i]
            
            if candle_3['low'] <= candle_1['high']:
                continue
            
            gap_percentage = (candle_3['low'] - candle_1['high']) / candle_1['high']
            if gap_percentage < FVG_MIN_GAP_PERCENTAGE:
                continue
            
            # MAè¿‘æ¥æ¡ä»¶ãƒã‚§ãƒƒã‚¯ï¼ˆbot_hwb.pyæ–¹å¼ï¼‰
            if not self.analyzer._check_fvg_ma_proximity(candle_3, candle_1):
                continue
            
            fvg = {
                'id': str(uuid.uuid4()),
                'setup_id': setup['id'],
                'formation_date': df_daily.index[i],
                'gap_percentage': gap_percentage,
                'lower_bound': candle_1['high'],
                'upper_bound': candle_3['low'],
                'status': 'active'
            }
            fvgs.append(fvg)
        
        return fvgs

    def _check_breakout_in_range(self, df_daily: pd.DataFrame, setup: Dict, fvg: Dict,
                                 start_idx: int, end_idx: int) -> Optional[Dict]:
        """æŒ‡å®šç¯„å›²å†…ã§ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆãƒã‚§ãƒƒã‚¯ï¼ˆRS Ratingè¿½åŠ ç‰ˆï¼‰"""
        try:
            setup_idx = df_daily.index.get_loc(setup['date'])
            fvg_idx = df_daily.index.get_loc(fvg['formation_date'])
        except KeyError:
            return None
        
        resistance_start_idx = setup_idx + 1
        resistance_end_idx = fvg_idx
        
        if resistance_end_idx <= resistance_start_idx:
            resistance_start_idx = max(0, setup_idx - 10)
            resistance_end_idx = setup_idx + 1
        
        resistance_data = df_daily.iloc[resistance_start_idx:resistance_end_idx]
        
        if resistance_data.empty:
            return None
        
        resistance_high = resistance_data['high'].max()
        
        for i in range(start_idx, end_idx):
            current = df_daily.iloc[i]
            if current['close'] > resistance_high * (1 + BREAKOUT_THRESHOLD):
                breakout_date = df_daily.index[i]

                # å‡ºæ¥é«˜å¢—åŠ ç‡ã‚’è¨ˆç®—
                volume_metrics = self.analyzer._calculate_volume_increase_at_date(df_daily, breakout_date)

                result = {
                    'status': 'breakout',
                    'breakout_date': breakout_date,
                    'breakout_price': current['close'],
                    'resistance_price': resistance_high,
                    'breakout_percentage': (current['close'] / resistance_high - 1) * 100
                }

                # å‡ºæ¥é«˜æƒ…å ±ã‚’è¿½åŠ 
                if volume_metrics:
                    result['breakout_volume'] = volume_metrics['breakout_volume']
                    result['avg_volume_20d'] = volume_metrics['avg_volume_20d']
                    result['volume_increase_pct'] = volume_metrics['volume_increase_pct']

                return result

        return None

    def _create_summary_from_data(self, symbol: str, signals: list, fvgs: list,
                                 latest_market_date: datetime.date) -> List[Dict]:
        """ã‚·ã‚°ãƒŠãƒ«ã¨FVGã‹ã‚‰ã‚µãƒãƒªãƒ¼ä½œæˆï¼ˆRS Ratingä¿æŒï¼‰"""
        summary_results = []
        today = latest_market_date
        
        business_days_back = 0
        current_date = today
        while business_days_back < 5:
            current_date -= timedelta(days=1)
            if current_date.weekday() < 5:
                business_days_back += 1
        five_business_days_ago = current_date

        for signal in signals:
            breakout_date_str = signal.get('breakout_date')
            if breakout_date_str:
                try:
                    breakout_date = pd.to_datetime(breakout_date_str).date()

                    summary_item = {
                        "symbol": symbol,
                        "signal_type": "",
                        "category": ""
                    }

                    # âœ… RS Ratingã‚’å«ã‚ã‚‹
                    if 'rs_rating' in signal:
                        summary_item['rs_rating'] = signal['rs_rating']

                    # âœ… å‡ºæ¥é«˜æƒ…å ±ã‚’å«ã‚ã‚‹
                    if 'volume_increase_pct' in signal:
                        summary_item['volume_increase_pct'] = signal['volume_increase_pct']
                    if 'breakout_volume' in signal:
                        summary_item['breakout_volume'] = signal['breakout_volume']
                    if 'avg_volume_20d' in signal:
                        summary_item['avg_volume_20d'] = signal['avg_volume_20d']

                    if breakout_date == today:
                        summary_item["signal_type"] = "signal_today"
                        summary_item["signal_date"] = breakout_date_str
                        summary_item["category"] = "å½“æ—¥ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆ"
                        summary_results.append(summary_item)
                    elif five_business_days_ago <= breakout_date < today:
                        summary_item["signal_type"] = "signal_recent"
                        summary_item["signal_date"] = breakout_date_str
                        summary_item["category"] = "ç›´è¿‘5å–¶æ¥­æ—¥ä»¥å†…"
                        summary_results.append(summary_item)
                except Exception as e:
                    logger.warning(f"Failed to parse breakout_date for {symbol}: {e}")
        
        for fvg in fvgs:
            if fvg.get('status') == 'active':
                formation_date_str = fvg.get('formation_date')
                if formation_date_str:
                    try:
                        formation_date = pd.to_datetime(formation_date_str).date()

                        if five_business_days_ago <= formation_date <= today:
                            summary_results.append({
                                "symbol": symbol,
                                "signal_type": "candidate",
                                "fvg_date": formation_date_str,
                                "category": "ç›£è¦–éŠ˜æŸ„"
                            })
                    except Exception as e:
                        logger.warning(f"Failed to parse formation_date for {symbol}: {e}")
        
        return summary_results

    def _create_summary_from_existing(self, existing_data: dict, latest_market_date: datetime.date) -> List[Dict]:
        """
        æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ€æ–°ã®æ—¥ä»˜åŸºæº–ã§ã‚µãƒãƒªãƒ¼ã‚’å†ä½œæˆ

        Args:
            existing_data: æ—¢å­˜ã®éŠ˜æŸ„åˆ†æãƒ‡ãƒ¼ã‚¿
            latest_market_date: æœ€æ–°ã®å¸‚å ´æ—¥ä»˜

        Returns:
            ã‚µãƒãƒªãƒ¼ãƒªã‚¹ãƒˆ
        """
        signals = existing_data.get('signals', [])
        fvgs = existing_data.get('fvgs', [])
        symbol = existing_data.get('symbol', 'UNKNOWN')

        # _create_summary_from_dataã‚’å†åˆ©ç”¨
        return self._create_summary_from_data(symbol, signals, fvgs, latest_market_date)

    def _save_symbol_data_with_chart(self, symbol: str, symbol_data: dict,
                                 df_daily: pd.DataFrame, df_weekly: pd.DataFrame):
        """
        ã‚·ãƒ³ãƒœãƒ«ãƒ‡ãƒ¼ã‚¿ã¨ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        """
        try:
            # ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
            chart_data = self._generate_lightweight_chart_data(symbol_data, df_daily, df_weekly)
            symbol_data['chart_data'] = chart_data

            # ãƒ‡ãƒ¼ã‚¿ä¿å­˜
            self.data_manager.save_symbol_data(symbol, symbol_data)
            logger.info(f"âœ… Saved data for {symbol}")

            # é™çš„ãƒãƒ£ãƒ¼ãƒˆç”»åƒã®ç”Ÿæˆï¼ˆå¯¾è±¡éŠ˜æŸ„ã®ã¿ï¼‰
            self._generate_static_chart_if_needed(symbol, symbol_data, df_daily)

        except Exception as e:
            logger.error(f"Failed to save data for {symbol}: {e}", exc_info=True)

    def _generate_static_chart_if_needed(self, symbol: str, symbol_data: dict, df_daily: pd.DataFrame):
        """
        å¯¾è±¡ã‚«ãƒ†ã‚´ãƒªï¼ˆå½“æ—¥ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¢ã‚¦ãƒˆã€ç›´è¿‘5å–¶æ¥­æ—¥ã€ç›£è¦–éŠ˜æŸ„ï¼‰ã«å«ã¾ã‚Œã‚‹å ´åˆã€
        é™çš„ãªãƒãƒ£ãƒ¼ãƒˆç”»åƒã‚’ç”Ÿæˆã™ã‚‹ã€‚
        """
        try:
            latest_date = df_daily.index[-1].date()

            # 5å–¶æ¥­æ—¥å‰ã‚’è¨ˆç®—ï¼ˆç°¡æ˜“çš„ï¼‰
            # æ­£ç¢ºã«ã¯ _create_summary_from_data ã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ãŒå¿…è¦ã ãŒã€
            # ã“ã“ã§ã¯ç›´è¿‘10æ—¥(æš¦æ—¥)ä»¥å†…ç¨‹åº¦ã§åˆ¤å®šã—ã¦ç”Ÿæˆã—ã¦ãŠãï¼ˆåºƒã‚ã«ç”Ÿæˆã—ã¦ã‚‚å•é¡Œãªã„ï¼‰
            check_threshold_date = latest_date - timedelta(days=10)

            is_target = False

            # ã‚·ã‚°ãƒŠãƒ«ãƒã‚§ãƒƒã‚¯ (å½“æ—¥ or ç›´è¿‘)
            for s in symbol_data.get('signals', []):
                if 'breakout_date' in s:
                    b_date = pd.to_datetime(s['breakout_date']).date()
                    if b_date >= check_threshold_date:
                        is_target = True
                        break

            # ç›£è¦–éŠ˜æŸ„ãƒã‚§ãƒƒã‚¯ (FVG)
            if not is_target:
                for f in symbol_data.get('fvgs', []):
                    if f.get('status') == 'active' and 'formation_date' in f:
                        f_date = pd.to_datetime(f['formation_date']).date()
                        if f_date >= check_threshold_date:
                            is_target = True
                            break

            if is_target:
                output_dir = os.path.join(os.path.dirname(__file__), '..', 'frontend', 'charts')
                generate_stock_chart(symbol, df_daily, output_dir, symbol_data)
                logger.debug(f"Generated chart for {symbol}")

        except Exception as e:
            logger.error(f"Failed to generate static chart for {symbol}: {e}")

    def _generate_lightweight_chart_data(self, symbol_data: dict, df_daily: pd.DataFrame, df_weekly: pd.DataFrame) -> dict:
        """ãƒãƒ£ãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        df_plot = df_daily.copy()
        df_plot['weekly_sma200_val'] = df_weekly['sma200'].reindex(df_plot.index, method='ffill')

        def format_series(df, col):
            s = df[[col]].dropna()
            return [{"time": i.strftime('%Y-%m-%d'), "value": r[col]} for i, r in s.iterrows()]

        def clean_np_types(d):
            for k, v in d.items():
                if isinstance(v, (np.int64, np.int32)):
                    d[k] = int(v)
                if isinstance(v, (np.float64, np.float32)):
                    d[k] = float(v)
            return d

        candles = [{
            "time": i.strftime('%Y-%m-%d'),
            "open": r.open,
            "high": r.high,
            "low": r.low,
            "close": r.close
        } for i, r in df_plot.iterrows()]

        volume_data = []
        for i, r in df_plot.iterrows():
            color = '#26a69a' if r['close'] >= r['open'] else '#ef5350'
            volume_data.append({
                "time": i.strftime('%Y-%m-%d'),
                "value": r['volume'],
                "color": color
            })

        markers = []

        for fvg in symbol_data.get('fvgs', []):
            try:
                formation_date = pd.to_datetime(fvg['formation_date'])
                if formation_date in df_plot.index:
                    formation_idx = df_plot.index.get_loc(formation_date)
                    if formation_idx >= 1:
                        middle_candle_date = df_plot.index[formation_idx - 1]
                        color_map = {
                            'active': '#FFD700',
                            'consumed': '#9370DB',
                            'violated': '#808080'
                        }
                        markers.append({
                            "time": middle_candle_date.strftime('%Y-%m-%d'),
                            "position": "inBar",
                            "color": color_map.get(fvg.get('status'), '#FFD700'),
                            "shape": "circle",
                            "text": "ğŸ®"
                        })
            except Exception as e:
                logger.warning(f"FVGãƒãƒ¼ã‚«ãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {symbol_data.get('symbol', 'N/A')} - {e}")

        for s in symbol_data.get('signals', []):
            markers.append({
                "time": s['breakout_date'],
                "position": "belowBar",
                "color": "#FF00FF",
                "shape": "arrowUp",
                "text": "Break"
            })

        return {
            'candles': candles,
            'sma200': format_series(df_plot, 'sma200'),
            'ema200': format_series(df_plot, 'ema200'),
            'weekly_sma200': format_series(df_plot, 'weekly_sma200_val'),
            'volume': [clean_np_types(v) for v in volume_data],
            'markers': [clean_np_types(m) for m in markers]
        }

    def _create_daily_summary(self, results: List[Dict], total_scanned: int, start_time: datetime) -> Dict:
        """æ—¥æ¬¡ã‚µãƒãƒªãƒ¼ä½œæˆï¼ˆ3ã‚«ãƒ†ã‚´ãƒªå¯¾å¿œã€ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å‰Šé™¤ï¼‰"""
        end_time = datetime.now()

        def _merge_and_sort(items: List[Dict], date_key: str) -> List[Dict]:
            """é‡è¤‡ã‚’é™¤å»ã—ã¦ã‚½ãƒ¼ãƒˆï¼ˆã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å‰Šé™¤ï¼‰"""
            merged = {}
            for item in items:
                if date_key not in item or 'symbol' not in item:
                    continue
                key = (item['symbol'], item[date_key])
                if key not in merged:
                    merged[key] = item
            # ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°å‰Šé™¤ï¼šã‚·ãƒ³ãƒœãƒ«åã§ã‚½ãƒ¼ãƒˆ
            return sorted(list(merged.values()), key=lambda x: x.get('symbol', ''))

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«åˆ†é¡
        signals_today = [r for r in results if r.get('signal_type') == 'signal_today']
        signals_recent = [r for r in results if r.get('signal_type') == 'signal_recent']
        candidates = [r for r in results if r.get('signal_type') == 'candidate']

        unique_signals_today = _merge_and_sort(signals_today, 'signal_date')
        unique_signals_recent = _merge_and_sort(signals_recent, 'signal_date')
        unique_candidates = _merge_and_sort(candidates, 'fvg_date')
        
        return {
            "scan_date": end_time.strftime('%Y-%m-%d'),
            "scan_time": end_time.strftime('%H:%M:%S'),
            "scan_duration_seconds": (end_time - start_time).total_seconds(),
            "total_scanned": total_scanned,
            "summary": {
                "signals_today_count": len(unique_signals_today),
                "signals_recent_count": len(unique_signals_recent),
                "candidates_count": len(unique_candidates),
                "signals_today": unique_signals_today,
                "signals_recent": unique_signals_recent,
                "candidates": unique_candidates
            },
            "performance": {
                "avg_time_per_symbol_ms": ((end_time - start_time).total_seconds() / total_scanned * 1000) if total_scanned > 0 else 0
            }
        }


async def run_hwb_scan(progress_callback=None):
    """ã‚¹ã‚­ãƒ£ãƒ³å®Ÿè¡Œã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ"""
    scanner = HWBScanner()
    summary = await scanner.scan_all_symbols(progress_callback)
    
    logger.info(
        f"å®Œäº† - å½“æ—¥: {summary['summary']['signals_today_count']}, "
        f"ç›´è¿‘: {summary['summary']['signals_recent_count']}, "
        f"ç›£è¦–: {summary['summary']['candidates_count']}"
    )
    return summary


async def analyze_single_ticker(symbol: str) -> Optional[Dict]:
    """å˜ä¸€éŠ˜æŸ„åˆ†æ"""
    scanner = HWBScanner()
    scanner._analyze_and_save_symbol(symbol)
    return scanner.data_manager.load_symbol_data(symbol)